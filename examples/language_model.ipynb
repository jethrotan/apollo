{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import os\n",
    "apollo_root = os.environ['APOLLO_ROOT']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import apollo\n",
    "import logging\n",
    "from apollo import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyper():\n",
    "    hyper = {}\n",
    "    hyper['vocab_size'] = 10000\n",
    "    hyper['batch_size'] = 32\n",
    "    hyper['init_range'] = 0.1\n",
    "    hyper['zero_symbol'] = hyper['vocab_size'] - 1\n",
    "    hyper['unknown_symbol'] = hyper['vocab_size'] - 2\n",
    "    hyper['test_interval'] = 100\n",
    "    hyper['test_iter'] = 20\n",
    "    hyper['base_lr'] = 20\n",
    "    hyper['weight_decay'] = 0\n",
    "    hyper['momentum'] = 0.0\n",
    "    hyper['clip_gradients'] = 0.24\n",
    "    hyper['display_interval'] = 20\n",
    "    hyper['max_iter'] = 2000000\n",
    "    hyper['snapshot_prefix'] = '/tmp/lm'\n",
    "    hyper['snapshot_interval'] = 10000\n",
    "    hyper['random_seed'] = 22\n",
    "    hyper['gamma'] = 0.792\n",
    "    hyper['stepsize'] = 10000\n",
    "    hyper['mem_cells'] = 250\n",
    "    hyper['graph_interval'] = 1000\n",
    "    hyper['graph_prefix'] = ''\n",
    "    return hyper\n",
    "\n",
    "hyper = get_hyper()\n",
    "\n",
    "apollo.Caffe.set_random_seed(hyper['random_seed'])\n",
    "apollo.Caffe.set_mode_gpu()\n",
    "apollo.Caffe.set_device(1)\n",
    "apollo.Caffe.set_logging_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # You can download this file with bash ./data/language_model/get_lm.sh\n",
    "    data_source = '%s/data/language_model/train_indices.txt' % apollo_root\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        with open(data_source, 'r') as f:\n",
    "            for x in f.readlines():\n",
    "                yield x.strip().split(' ')\n",
    "        logging.info('epoch %s finished' % epoch)\n",
    "        epoch += 1\n",
    "    \n",
    "def pad_batch(sentence_batch):\n",
    "    max_len = max(len(x) for x in sentence_batch)\n",
    "    result = []\n",
    "    for x in sentence_batch:\n",
    "        y = [int(z) if int(z) < hyper['unknown_symbol'] else hyper['unknown_symbol']\n",
    "            for z in x]\n",
    "        result.append(y + [hyper['zero_symbol']] * (max_len - len(x)))\n",
    "    return result\n",
    "    \n",
    "def get_data_batch(data_iter):\n",
    "    while True:\n",
    "        raw_batch = []\n",
    "        for i in range(hyper['batch_size']):\n",
    "            raw_batch.append(next(data_iter))\n",
    "        sentence_batch = np.array(pad_batch(raw_batch))\n",
    "        yield sentence_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(net, sentence_batches):\n",
    "    sentence_batch = next(sentence_batches)\n",
    "    length = min(sentence_batch.shape[1], 30)\n",
    "\n",
    "    filler = layers.Filler(type='uniform', max=hyper['init_range'],\n",
    "        min=(-hyper['init_range']))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_seed',\n",
    "        data=np.zeros((hyper['batch_size'], hyper['mem_cells'], 1, 1))))\n",
    "    net.forward_layer(layers.NumpyData(name='label',\n",
    "        data=np.zeros((hyper['batch_size'] * length, 1, 1, 1))))\n",
    "    hidden_concat_bottoms = []\n",
    "    for step in range(length):\n",
    "        net.forward_layer(layers.DummyData(name=('word%d' % step),\n",
    "            shape=[hyper['batch_size'], 1, 1, 1]))\n",
    "        if step == 0:\n",
    "            prev_hidden = 'lstm_seed'\n",
    "            prev_mem = 'lstm_seed'\n",
    "            word = np.zeros(sentence_batch[:, 0].shape)\n",
    "        else:\n",
    "            prev_hidden = 'lstm%d_hidden' % (step - 1)\n",
    "            prev_mem = 'lstm%d_mem' % (step - 1)\n",
    "            word = sentence_batch[:, step - 1]\n",
    "        net.tops['word%d' % step].data[:,0,0,0] = word\n",
    "        net.forward_layer(layers.Wordvec(name=('wordvec%d' % step),\n",
    "            bottoms=['word%d' % step],\n",
    "            dimension=hyper['mem_cells'], vocab_size=hyper['vocab_size'],\n",
    "            param_names=['wordvec_param'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Concat(name='lstm_concat%d' % step,\n",
    "            bottoms=[prev_hidden, 'wordvec%d' % step]))\n",
    "        net.forward_layer(layers.Lstm(name='lstm%d' % step,\n",
    "            bottoms=['lstm_concat%d' % step, prev_mem],\n",
    "            param_names=['lstm_input_value', 'lstm_input_gate',\n",
    "                'lstm_forget_gate', 'lstm_output_gate'],\n",
    "            tops=['lstm%d_hidden' % step, 'lstm%d_mem' % step],\n",
    "            num_cells=hyper['mem_cells'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Dropout(name='dropout%d' % step,\n",
    "            bottoms=['lstm%d_hidden' % step], dropout_ratio=0.16))\n",
    "        hidden_concat_bottoms.append('dropout%d' % step)\n",
    "\n",
    "    net.forward_layer(layers.Concat(name='hidden_concat',\n",
    "        concat_dim=0, bottoms=hidden_concat_bottoms))\n",
    "    net.tops['label'].data[:,0,0,0] = sentence_batch[:, :length].T.flatten()\n",
    "    net.forward_layer(layers.InnerProduct(name='ip', bottoms=['hidden_concat'],\n",
    "        num_output=hyper['vocab_size'], weight_filler=filler))\n",
    "    loss = net.forward_layer(layers.SoftmaxWithLoss(name='softmax_loss',\n",
    "        ignore_label=hyper['zero_symbol'], bottoms=['ip', 'label']))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = apollo.Net()\n",
    "\n",
    "apollo.log.log_to_stdout() # for ipython notebook\n",
    "sentences = get_data()\n",
    "sentence_batches = get_data_batch(sentences)\n",
    "\n",
    "forward(net, sentence_batches)\n",
    "net.reset_forward()\n",
    "train_loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-05 13:17:39,582 - INFO - Iteration 0: 6.8161034584\n",
      "2015-07-05 13:17:42,248 - INFO - Iteration 20: 6.49280529022\n",
      "2015-07-05 13:17:44,842 - INFO - Iteration 40: 6.1886277914\n",
      "2015-07-05 13:17:47,437 - INFO - Iteration 60: 6.03695037365\n",
      "2015-07-05 13:17:50,030 - INFO - Iteration 80: 5.94959592819\n",
      "2015-07-05 13:17:52,626 - INFO - Iteration 100: 5.79375097752\n",
      "2015-07-05 13:17:55,221 - INFO - Iteration 120: 5.88647036552\n",
      "2015-07-05 13:17:57,818 - INFO - Iteration 140: 5.76611590385\n",
      "2015-07-05 13:18:00,416 - INFO - Iteration 160: 5.68361911774\n",
      "2015-07-05 13:18:03,013 - INFO - Iteration 180: 5.67861757278\n",
      "2015-07-05 13:18:05,610 - INFO - Iteration 200: 5.574786973\n",
      "2015-07-05 13:18:08,204 - INFO - Iteration 220: 5.5921135664\n",
      "2015-07-05 13:18:10,799 - INFO - Iteration 240: 5.50135638714\n",
      "2015-07-05 13:18:13,394 - INFO - Iteration 260: 5.53342981339\n",
      "2015-07-05 13:18:15,990 - INFO - Iteration 280: 5.4502014637\n"
     ]
    }
   ],
   "source": [
    "for i in range(hyper['max_iter']):\n",
    "    train_loss_hist.append(forward(net, sentence_batches))\n",
    "    net.backward()\n",
    "    lr = (hyper['base_lr'] * (hyper['gamma'])**(i // hyper['stepsize']))\n",
    "    net.update(lr=lr, momentum=hyper['momentum'],\n",
    "        clip_gradients=hyper['clip_gradients'], weight_decay=hyper['weight_decay'])\n",
    "    if i % hyper['display_interval'] == 0:\n",
    "        logging.info('Iteration %d: %s' % (i, np.mean(train_loss_hist[-hyper['display_interval']:])))\n",
    "    if i % hyper['test_interval'] == 0:\n",
    "        #test_performance(net, test_net)\n",
    "        pass\n",
    "    if i % hyper['snapshot_interval'] == 0 and i > 0:\n",
    "        filename = '%s_%d.h5' % (hyper['snapshot_prefix'], i)\n",
    "        logging.info('Saving net to: %s' % filename)\n",
    "        net.save(filename)\n",
    "    if i % hyper['graph_interval'] == 0 and i > 0:\n",
    "        sub = 100\n",
    "        plt.plot(np.convolve(train_loss_hist, np.ones(sub)/sub)[sub:-sub])\n",
    "        filename = '%strain_loss.jpg' % hyper['graph_prefix']\n",
    "        logging.info('Saving figure to: %s' % filename)\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_forward(net):\n",
    "    output_words = []\n",
    "    filler = layers.Filler(type='uniform', max=hyper['init_range'],\n",
    "        min=(-hyper['init_range']))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_hidden_prev',\n",
    "        data=np.zeros((1, hyper['mem_cells'], 1, 1))))\n",
    "    net.forward_layer(layers.NumpyData(name='lstm_mem_prev',\n",
    "        data=np.zeros((1, hyper['mem_cells'], 1, 1))))\n",
    "    length = 30\n",
    "    for step in range(length):\n",
    "        net.forward_layer(layers.NumpyData(name=('word'),\n",
    "            data=np.zeros((1, 1, 1, 1))))\n",
    "        prev_hidden = 'lstm_hidden_prev'\n",
    "        prev_mem = 'lstm_mem_prev'\n",
    "        word = np.zeros((1, 1, 1, 1))\n",
    "        if step == 0:\n",
    "            net.tops['word'].data[0,0,0,0] = random.randrange(1,100)\n",
    "        else:\n",
    "            output_words.append(np.argmax(net.tops['softmax'].data.flatten()[:9000]))\n",
    "            net.tops['word'].data[0,0,0,0] = np.argmax(net.tops['softmax'].data)\n",
    "        net.forward_layer(layers.Wordvec(name=('wordvec'),\n",
    "            bottoms=['word'],\n",
    "            dimension=hyper['mem_cells'], vocab_size=hyper['vocab_size'],\n",
    "            param_names=['wordvec_param'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Concat(name='lstm_concat',\n",
    "            bottoms=[prev_hidden, 'wordvec']))\n",
    "        net.forward_layer(layers.Lstm(name='lstm',\n",
    "            bottoms=['lstm_concat', prev_mem],\n",
    "            param_names=['lstm_input_value', 'lstm_input_gate',\n",
    "                'lstm_forget_gate', 'lstm_output_gate'],\n",
    "            tops=['lstm_hidden_next', 'lstm_mem_next'],\n",
    "            num_cells=hyper['mem_cells'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Dropout(name='dropout',\n",
    "            bottoms=['lstm_hidden_next'], dropout_ratio=0.16))\n",
    "\n",
    "        net.forward_layer(layers.InnerProduct(name='ip', bottoms=['dropout'],\n",
    "            num_output=hyper['vocab_size'], weight_filler=filler))\n",
    "        net.forward_layer(layers.Softmax(name='softmax',\n",
    "            ignore_label=hyper['zero_symbol'], bottoms=['ip']))\n",
    "        net.tops['lstm_hidden_prev'].data_tensor.copy_from(net.tops['lstm_hidden_next'].data_tensor)\n",
    "        net.tops['lstm_mem_prev'].data_tensor.copy_from(net.tops['lstm_mem_next'].data_tensor)\n",
    "        net.reset_forward()\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "with open('%s/data/language_model/vocab.pkl' % os.environ['APOLLO_ROOT'], 'r') as f:\n",
    "    vocab = pickle.load(f)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_net = apollo.Net()\n",
    "eval_forward(eval_net)\n",
    "eval_net.load('%s_20000.h5' % hyper['snapshot_prefix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_words = eval_forward(eval_net)\n",
    "print ' '.join([inv_vocab[x] for x in output_words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
